https://blog.csdn.net/weixin_45901519/article/details/106915960

对epoch，batch size, iterations的理解

Ma Sizhou 2020-06-23 11:09:14  67  收藏
分类专栏： 深度学习 文章标签： 深度学习
版权
首先要明确这几个都是超参数，也就是要人为的调。
目录
一、概念
1、Epoch
2、Batch Size
3、iterations
二、Epoch、Batch Size出现的原因
1、Epoch
2、Batch Size
三、如何选取合适的Epoch、Batch Size
1、Epoch的选取
2、Batch Size的选取
四、 问题
一、概念
1、Epoch
one epoch：所有的训练样本完成一次Forword运算以及一次BP运算。
epoch：使用训练集的全部数据对模型进行一次完整训练。

2、Batch Size
batch：在一个epoch里，把训练数据分成几份，一份就叫做一个batch。
batch size：1个batch包含的样本的数目，也就是每份里样本的个数。

3、iterations
所谓iterations就是完成一次epoch所需的batch个数。

二、Epoch、Batch Size出现的原因
1、Epoch
其实epoch好理解，就是说我们在训练模型的时候要把train data一遍又一遍的放入模型中来更新我们的参数，使得模型的参数合理。这个就像你小时候手里拿着面包，但是你不知道是什么东西，你问了你妈妈一次，但是你没记住，又问了一次，还没记住，问了100次，终于记住了。但是你问了1000次，然后看到别的东西，你都认为是面包，这个就是epoch太大，过拟合了。

2、Batch Size
下面看看不好理解的Batch Size。在此之前先看看几种深度学习的优化算法，也就是梯度下降算法，梯度下降分为三种：

（1）批量梯度下降算法（BGD，Batch gradient descent algorithm）
批量梯度下降算法，每一次计算都需要遍历全部数据集，更新梯度，其一、计算开销大，花费时间长，不支持在线学习。其二、随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行。

（2）随机梯度下降算法（SGD，Stochastic gradient descent algorithm）
随机梯度下降算法，每次随机选取一条数据，求梯度更新参数，这就是在线学习（Online Learning）。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。如图所示：
在这里插入图片描述

（3）小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）

为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。

显然这里如果把batch_size设置为train data的长度，就成了批量梯度下降算法，batch_size设置为1就是随机梯度下降算法。

三、如何选取合适的Epoch、Batch Size
1、Epoch的选取
Epoch的选取太小会欠拟合，太大又会过拟合，所以这个超参数的选取看了许多文章，一个好的选择应该是：设一个验证集，选取验证集最好的Epoch。

2、Batch Size的选取
（1）当有足够算力时，选取batch size为32或更小一些。
（2）算力不够时，在效率和泛化性之间做trade-off，尽量选择更小的batch size。
（3）当模型训练到尾声，想更精细化地提高成绩（比如论文实验/比赛到最后），有一个有用的trick，就是设置batch size为1，即做纯SGD，慢慢把error磨低。

四、 问题
虽然前面说过把batch_size设置为train data的长度，就成了批量梯度下降算法，就会有当数据集太大时，内存撑不住。那么，如果真有一个怪兽级显卡，使得一次计算10000个样本跟计算1个样本的时间相同的话，是不是设置10000就一定是最好的呢？虽然从收敛速度上来说是的，但是我们知道，神经网络是个复杂的model，它的损失函数也不是省油的灯，在实际问题中，神经网络的loss曲面（以model参数为自变量，以loss值为因变量画出来的曲面）往往是非凸的，这意味着很可能有多个局部最优点，而且很可能有鞍点！插播一下，鞍点就是loss曲面中像马鞍一样形状的地方的中心点（其实就是此时用梯度下降法更新参数的时候，梯度是0，更新不了参数，如果是一维空间，就是这一点是水平的），如下图：
在这里插入图片描述
想象一下，在鞍点处，横着看的话，鞍点就是个极小值点，但是竖着看的话，鞍点就是极大值点，因此鞍点容易给优化算法一个“我已经收敛了”的假象，殊不知其旁边有一个可以跳下去的万丈深渊。

我看了很多文章，有人说batch size越大则会陷入局部最优解，但有人评论说越大则更容易找到全局最优解，按我的理解来说，这个要看也损失函数本身是不是凸函数，如果是凸函数batch size越大越容易收敛，且找到的是全局最优解，而如果不是凸函数，那么就像前面说的可能有鞍点！这是会陷入局部最优解的。

参考文献：

（1）: 链接.
（2）: 链接.
（3）: 链接.
